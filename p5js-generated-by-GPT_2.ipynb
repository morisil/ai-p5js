{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of GPT-2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morisil/ai-p5js/blob/main/p5js-generated-by-GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzxl1vYX-1kk"
      },
      "source": [
        "Setup:\n",
        "\n",
        "1) Make sure GPU is enabled, go to edit->notebook settings->Hardware Accelerator GPU\n",
        "\n",
        "2) Make a copy to your google drive, click on copy to drive in panel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW0abT07ZkhZ"
      },
      "source": [
        "Note: Colab will reset after 12 hours make sure to save your model checkpoints to google drive around 10-11 hours mark or before, then go to runtime->reset all runtimes. Now copy your train model back into colab and start training again from the previous checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXW02eIYpcB"
      },
      "source": [
        "clone and cd into repo, nshepperd's fork https://github.com/nshepperd/gpt-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e608c74-52d4-480d-a1c2-1f419e389ac9"
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 435, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/64)\u001b[K\rremote: Counting objects:   3% (2/64)\u001b[K\rremote: Counting objects:   4% (3/64)\u001b[K\rremote: Counting objects:   6% (4/64)\u001b[K\rremote: Counting objects:   7% (5/64)\u001b[K\rremote: Counting objects:   9% (6/64)\u001b[K\rremote: Counting objects:  10% (7/64)\u001b[K\rremote: Counting objects:  12% (8/64)\u001b[K\rremote: Counting objects:  14% (9/64)\u001b[K\rremote: Counting objects:  15% (10/64)\u001b[K\rremote: Counting objects:  17% (11/64)\u001b[K\rremote: Counting objects:  18% (12/64)\u001b[K\rremote: Counting objects:  20% (13/64)\u001b[K\rremote: Counting objects:  21% (14/64)\u001b[K\rremote: Counting objects:  23% (15/64)\u001b[K\rremote: Counting objects:  25% (16/64)\u001b[K\rremote: Counting objects:  26% (17/64)\u001b[K\rremote: Counting objects:  28% (18/64)\u001b[K\rremote: Counting objects:  29% (19/64)\u001b[K\rremote: Counting objects:  31% (20/64)\u001b[K\rremote: Counting objects:  32% (21/64)\u001b[K\rremote: Counting objects:  34% (22/64)\u001b[K\rremote: Counting objects:  35% (23/64)\u001b[K\rremote: Counting objects:  37% (24/64)\u001b[K\rremote: Counting objects:  39% (25/64)\u001b[K\rremote: Counting objects:  40% (26/64)\u001b[K\rremote: Counting objects:  42% (27/64)\u001b[K\rremote: Counting objects:  43% (28/64)\rremote: Counting objects:  45% (29/64)\u001b[K\rremote: Counting objects:  46% (30/64)\u001b[K\rremote: Counting objects:  48% (31/64)\u001b[K\rremote: Counting objects:  50% (32/64)\u001b[K\rremote: Counting objects:  51% (33/64)\u001b[K\rremote: Counting objects:  53% (34/64)\u001b[K\rremote: Counting objects:  54% (35/64)\u001b[K\rremote: Counting objects:  56% (36/64)\u001b[K\rremote: Counting objects:  57% (37/64)\u001b[K\rremote: Counting objects:  59% (38/64)\u001b[K\rremote: Counting objects:  60% (39/64)\u001b[K\rremote: Counting objects:  62% (40/64)\u001b[K\rremote: Counting objects:  64% (41/64)\u001b[K\rremote: Counting objects:  65% (42/64)\u001b[K\rremote: Counting objects:  67% (43/64)\u001b[K\rremote: Counting objects:  68% (44/64)\u001b[K\rremote: Counting objects:  70% (45/64)\u001b[K\rremote: Counting objects:  71% (46/64)\u001b[K\rremote: Counting objects:  73% (47/64)\u001b[K\rremote: Counting objects:  75% (48/64)\u001b[K\rremote: Counting objects:  76% (49/64)\u001b[K\rremote: Counting objects:  78% (50/64)\u001b[K\rremote: Counting objects:  79% (51/64)\u001b[K\rremote: Counting objects:  81% (52/64)\u001b[K\rremote: Counting objects:  82% (53/64)\u001b[K\rremote: Counting objects:  84% (54/64)\u001b[K\rremote: Counting objects:  85% (55/64)\u001b[K\rremote: Counting objects:  87% (56/64)\u001b[K\rremote: Counting objects:  89% (57/64)\u001b[K\rremote: Counting objects:  90% (58/64)\u001b[K\rremote: Counting objects:  92% (59/64)\u001b[K\rremote: Counting objects:  93% (60/64)\u001b[K\rremote: Counting objects:  95% (61/64)\u001b[K\rremote: Counting objects:  96% (62/64)\u001b[K\rremote: Counting objects:  98% (63/64)\u001b[K\rremote: Counting objects: 100% (64/64)\u001b[K\rremote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 435 (delta 19), reused 48 (delta 13), pack-reused 371\u001b[K\n",
            "Receiving objects: 100% (435/435), 4.48 MiB | 22.81 MiB/s, done.\n",
            "Resolving deltas: 100% (220/220), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50175b7b-a54b-463d-ccc6-d41468f71730"
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtn1qZPgZLb0"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c264c633-1950-4376-d8a1-750e5f2e5001"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Collecting toposort==1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Installing collected packages: toposort\n",
            "Successfully installed toposort-1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvUQhgK3PQ4L"
      },
      "source": [
        "Mount drive to access google drive for saving and accessing checkpoints later. Have to log in to your google account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNpf6R4ahYSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897c858e-2ffe-4f69-d41b-37b26e1c8872"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1hrgeKFYsuE"
      },
      "source": [
        "Download the model data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A498TySgHYyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebf68fa-2a40-4dc0-f264-a96b775f59c9"
      },
      "source": [
        "!python3 download_model.py 117M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 1.09Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 5.73Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.17Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:41, 12.1Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.77Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 3.57Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 2.92Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UDpEGjfO8Q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf17ca7-3515-4439-d09b-ef0fe36e7721"
      },
      "source": [
        "!python3 download_model.py 345M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 1.00Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 5.07Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.09Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [03:14, 7.30Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 9.42Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 4.31Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 3.26Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq-YwRnNOBYO"
      },
      "source": [
        "encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJPQtdLbbeK"
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KzSbAvePgsI"
      },
      "source": [
        "Fetch checkpoints if you have them saved in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA2Wk7yIPmS6"
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/checkpoint/ /content/gpt-2/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p--9zwqQRTc"
      },
      "source": [
        "\n",
        "Let's get our train on! In this case the file is A Tale of Two Cities (Charles Dickens) from Project Gutenberg. To change the dataset GPT-2 models will fine-tune on, change this URL to another .txt file, and change corresponding part of the next cell. Note that you can use small datasets if you want but you will have to be sure not to run the fine-tuning for too long or you will overfit badly. Roughly, expect interesting results within minutes to hours in the 1-10s of megabyte ballpark, and below this you may want to stop the run early as fine-tuning can be very fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOCvrs-DHvxa"
      },
      "source": [
        "!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA4N1VufoLDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778471e6-0237-4404-b1f9-5ec446c9cf9a"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB1Zx__cnCpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243ae508-8857-46e3-ab0b-ed30b32a724e"
      },
      "source": [
        "\n",
        "!python3 encode.py --model_name 117M --models_dir ../models  FreudDream.txt Freud.npz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"encode.py\", line 9, in <module>\n",
            "    import encoder\n",
            "ModuleNotFoundError: No module named 'encoder'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfJ5b3CQXqr"
      },
      "source": [
        "\n",
        "Start training, add --model_name '345M' to use 345 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEn_ihcGI00T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0911b3-ea27-4638-9b9d-d5587704decf"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset src/Freud.npz --model_name '117M' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 17:01:49.102103: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-07 17:01:50.720582: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-05-07 17:01:50.721421: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-05-07 17:01:50.757779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:50.758365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-05-07 17:01:50.758405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-07 17:01:50.760587: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-07 17:01:50.760675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-05-07 17:01:50.762225: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-07 17:01:50.762598: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-07 17:01:50.764209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-07 17:01:50.764719: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-05-07 17:01:50.764899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-05-07 17:01:50.764991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:50.765583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:50.766130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-05-07 17:01:50.766619: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-05-07 17:01:50.766740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:50.767264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-05-07 17:01:50.767325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-07 17:01:50.767357: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-07 17:01:50.767379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-05-07 17:01:50.767398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-07 17:01:50.767417: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-07 17:01:50.767439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-07 17:01:50.767458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-05-07 17:01:50.767477: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-05-07 17:01:50.767551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:50.768171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:50.768713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-05-07 17:01:50.768763: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-07 17:01:51.277026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-07 17:01:51.277082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-05-07 17:01:51.277103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-05-07 17:01:51.277347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:51.277940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:51.278517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 17:01:51.279044: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-05-07 17:01:51.279094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13994 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:60: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "Using Adam optimizer\n",
            "2021-05-07 17:01:58.528679: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00, 138.79it/s]\n",
            "dataset has 76830 tokens\n",
            "Training...\n",
            "2021-05-07 17:02:01.911181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-07 17:02:02.331470: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "[1 | 1.28] loss=3.85 avg=3.85\n",
            "[2 | 1.69] loss=3.68 avg=3.76\n",
            "[3 | 2.09] loss=3.40 avg=3.64\n",
            "[4 | 2.51] loss=3.52 avg=3.61\n",
            "[5 | 2.92] loss=3.42 avg=3.57\n",
            "[6 | 3.33] loss=3.52 avg=3.56\n",
            "[7 | 3.75] loss=3.47 avg=3.55\n",
            "[8 | 4.16] loss=3.36 avg=3.52\n",
            "[9 | 4.58] loss=3.46 avg=3.52\n",
            "[10 | 4.99] loss=3.47 avg=3.51\n",
            "[11 | 5.40] loss=3.27 avg=3.49\n",
            "[12 | 5.82] loss=3.44 avg=3.48\n",
            "[13 | 6.23] loss=3.45 avg=3.48\n",
            "[14 | 6.65] loss=3.49 avg=3.48\n",
            "[15 | 7.06] loss=3.58 avg=3.49\n",
            "[16 | 7.48] loss=3.31 avg=3.48\n",
            "[17 | 7.89] loss=3.23 avg=3.46\n",
            "[18 | 8.31] loss=3.07 avg=3.44\n",
            "[19 | 8.72] loss=3.12 avg=3.42\n",
            "[20 | 9.14] loss=3.36 avg=3.42\n",
            "[21 | 9.56] loss=3.69 avg=3.43\n",
            "[22 | 9.97] loss=3.23 avg=3.42\n",
            "[23 | 10.39] loss=3.10 avg=3.41\n",
            "[24 | 10.80] loss=3.29 avg=3.40\n",
            "[25 | 11.22] loss=3.47 avg=3.40\n",
            "[26 | 11.63] loss=3.42 avg=3.40\n",
            "[27 | 12.05] loss=3.43 avg=3.41\n",
            "[28 | 12.47] loss=3.19 avg=3.40\n",
            "[29 | 12.88] loss=3.38 avg=3.40\n",
            "[30 | 13.30] loss=3.35 avg=3.39\n",
            "[31 | 13.72] loss=3.35 avg=3.39\n",
            "[32 | 14.13] loss=3.61 avg=3.40\n",
            "[33 | 14.55] loss=3.40 avg=3.40\n",
            "[34 | 14.97] loss=3.15 avg=3.39\n",
            "[35 | 15.39] loss=3.23 avg=3.39\n",
            "[36 | 15.81] loss=3.29 avg=3.38\n",
            "[37 | 16.22] loss=3.18 avg=3.38\n",
            "[38 | 16.64] loss=3.25 avg=3.37\n",
            "[39 | 17.06] loss=3.17 avg=3.37\n",
            "[40 | 17.48] loss=3.40 avg=3.37\n",
            "[41 | 17.90] loss=3.27 avg=3.36\n",
            "[42 | 18.32] loss=3.19 avg=3.36\n",
            "[43 | 18.74] loss=3.30 avg=3.36\n",
            "[44 | 19.16] loss=3.33 avg=3.36\n",
            "[45 | 19.58] loss=3.26 avg=3.35\n",
            "[46 | 20.00] loss=3.28 avg=3.35\n",
            "[47 | 20.42] loss=3.17 avg=3.35\n",
            "[48 | 20.84] loss=3.25 avg=3.35\n",
            "[49 | 21.26] loss=3.44 avg=3.35\n",
            "[50 | 21.68] loss=3.22 avg=3.34\n",
            "[51 | 22.10] loss=3.30 avg=3.34\n",
            "[52 | 22.52] loss=3.18 avg=3.34\n",
            "[53 | 22.94] loss=3.34 avg=3.34\n",
            "[54 | 23.36] loss=3.15 avg=3.33\n",
            "[55 | 23.78] loss=3.34 avg=3.34\n",
            "[56 | 24.20] loss=3.36 avg=3.34\n",
            "[57 | 24.62] loss=3.11 avg=3.33\n",
            "[58 | 25.05] loss=3.18 avg=3.33\n",
            "[59 | 25.47] loss=3.00 avg=3.32\n",
            "[60 | 25.89] loss=3.10 avg=3.31\n",
            "[61 | 26.31] loss=3.32 avg=3.31\n",
            "[62 | 26.73] loss=3.12 avg=3.31\n",
            "[63 | 27.16] loss=3.08 avg=3.31\n",
            "[64 | 27.58] loss=3.11 avg=3.30\n",
            "[65 | 28.00] loss=3.14 avg=3.30\n",
            "[66 | 28.43] loss=3.21 avg=3.30\n",
            "[67 | 28.85] loss=3.13 avg=3.29\n",
            "[68 | 29.28] loss=2.89 avg=3.29\n",
            "[69 | 29.70] loss=3.17 avg=3.28\n",
            "[70 | 30.12] loss=3.31 avg=3.28\n",
            "[71 | 30.55] loss=3.21 avg=3.28\n",
            "[72 | 30.97] loss=2.97 avg=3.28\n",
            "[73 | 31.40] loss=3.32 avg=3.28\n",
            "[74 | 31.82] loss=3.40 avg=3.28\n",
            "[75 | 32.25] loss=3.17 avg=3.28\n",
            "[76 | 32.67] loss=3.32 avg=3.28\n",
            "[77 | 33.10] loss=3.44 avg=3.28\n",
            "[78 | 33.52] loss=3.02 avg=3.28\n",
            "[79 | 33.95] loss=2.99 avg=3.27\n",
            "[80 | 34.38] loss=3.09 avg=3.27\n",
            "[81 | 34.80] loss=3.19 avg=3.27\n",
            "[82 | 35.23] loss=3.34 avg=3.27\n",
            "[83 | 35.66] loss=3.29 avg=3.27\n",
            "[84 | 36.09] loss=2.77 avg=3.26\n",
            "[85 | 36.51] loss=3.22 avg=3.26\n",
            "[86 | 36.94] loss=3.38 avg=3.26\n",
            "[87 | 37.37] loss=2.82 avg=3.25\n",
            "[88 | 37.80] loss=3.29 avg=3.25\n",
            "[89 | 38.22] loss=3.28 avg=3.25\n",
            "[90 | 38.65] loss=3.06 avg=3.25\n",
            "[91 | 39.08] loss=3.17 avg=3.25\n",
            "[92 | 39.51] loss=3.28 avg=3.25\n",
            "[93 | 39.94] loss=2.96 avg=3.25\n",
            "[94 | 40.37] loss=3.03 avg=3.24\n",
            "[95 | 40.80] loss=3.24 avg=3.24\n",
            "[96 | 41.23] loss=2.93 avg=3.24\n",
            "[97 | 41.66] loss=2.98 avg=3.23\n",
            "[98 | 42.09] loss=3.16 avg=3.23\n",
            "[99 | 42.52] loss=3.36 avg=3.23\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "bed this morning were the words\n",
            "\n",
            "I wrote when I was very little. And this morning I was like\n",
            "\n",
            "that dreamt of the past. He is coming along; now that I\n",
            "\n",
            "know what he will do to me, I am like him. The\n",
            "\n",
            "dreams for which I wished I could not be taken\n",
            "\n",
            "to my bed are like the longing that follows the dreams\n",
            "\n",
            "of the past. No dreamer shall take anything more\n",
            "\n",
            "unwilling than to take a step back, and yet it shall have no\n",
            "\n",
            "effect on that dreamer either.\n",
            "\n",
            "Now that I have thought about the dream, I cannot but suspect\n",
            "the dream has received a great deal of attention from\n",
            "this day; indeed I have told myself that he is much better\n",
            "than I, for I may prove my suspicion as a false\n",
            "solution. I think of the dream and its dream-soul as\n",
            "a kind of a dream-song, and all of this may be the product of this.\n",
            "\n",
            "A man has not yet made me to look forward to the day\n",
            "when I shall, in my own case, become so conscious of\n",
            "the dream as to say, 'I cannot hope for things I have not\n",
            "been able to dream . . . For by day I think of one dream\n",
            "that has been lost in me.' If it were this\n",
            "I should have known would be quite incomprehensible to an ordinary man\n",
            "and all the dream ideas of this day would have\n",
            "to be forgotten in my memory. But I wish now to\n",
            "remember what I used to dread and how it has taken me\n",
            "to have my doubts, to think about what I once considered\n",
            "as true thoughts, and to be reminded of those who have gone before me.\n",
            "\n",
            "I also fear my dream is still going on, because all I have of my dream\n",
            "to dream-song has been lost. Now the fear is not so severe\n",
            "as in the dream, and I can be sure the dream dreams have not gone\n",
            "on even as far as they went down to the end of life.\n",
            "\n",
            "I had been afraid all my life that I should be so near to a dream I\n",
            " would not dream again, for there would be no longer an end in sight,\n",
            "and that I would have been compelled to see the dream of the last\n",
            "month of my life. In this dream I remember every\n",
            "childlike dream of my childhood. This dream is so like\n",
            "their dreams that it should have not been forgotten in my memory.\n",
            "\n",
            "And here I was. I was always afraid of the thoughts that I could not\n",
            "be quite certain of. It was too far down the memory rabbit\n",
            "path to know what I felt. Yet at the very moment when I felt the need to\n",
            "be at the same time aware of myself, I thought that I had a wish which I should\n",
            "feel at that time to have the wish taken away from me to take away. If I\n",
            "had done this I would have dreamed it, and I could have made a very hard\n",
            "out case that the wish I wished could be taken away. My only hope for\n",
            "being deprived of the wish I wished was to go to sleep on the\n",
            "wish of a dream as a child, or to get the wish taken from me.\n",
            "\n",
            "If I had to do this I would have gone to sleep on the wish to get\n",
            "the wish taken away from me, but as I had always had\n",
            "numerous or more dreams, and had never been so afraid, and that wish became just so\n",
            "tense that I would not dream again, I could very easily dream that, but I\n",
            "would have dreamed only to see it taken away from me. This wish has become much\n",
            "obscene.\n",
            "\n",
            "Of all the dreams about me I must mention that I dream about a very little dream\n",
            "that I dreamed about a long time ago. And I know of only one\n",
            "dreams of my life that I would not dream again.\n",
            "\n",
            "I remember also the dream of a young girl who left me\n",
            "with four brothers. When I went to find them there was a little boy\n",
            "and his father in the garden. We did not know their names either ;\n",
            "they were named M. Iyer, M. Leyden, and M. B. Hickey. He was the only one who\n",
            "had ever been with me in any of the dreams described; and yet there was a dream\n",
            "that I have always regretted.\n",
            "\n",
            "A little boy, who seems to me a more charming person than I have remembered,\n",
            "has become quite a stranger to me. It is an extremely interesting dream,\n",
            "\n",
            "but I do not know about other such dreams I would wish for. It is strange that in such an\n",
            "intimate manner the dream which I dream of should not be taken away from\n",
            "me even as a child's wish, and which I should dream\n",
            "of only as a dream dream, when I have been in such a\n",
            "\n",
            "[100 | 55.04] loss=3.15 avg=3.23\n",
            "[101 | 55.49] loss=3.14 avg=3.23\n",
            "[102 | 55.92] loss=2.84 avg=3.22\n",
            "[103 | 56.35] loss=3.20 avg=3.22\n",
            "[104 | 56.79] loss=3.25 avg=3.22\n",
            "[105 | 57.22] loss=3.14 avg=3.22\n",
            "[106 | 57.66] loss=3.16 avg=3.22\n",
            "[107 | 58.10] loss=3.04 avg=3.22\n",
            "[108 | 58.54] loss=3.35 avg=3.22\n",
            "[109 | 58.97] loss=2.88 avg=3.22\n",
            "[110 | 59.41] loss=3.08 avg=3.21\n",
            "[111 | 59.84] loss=2.89 avg=3.21\n",
            "[112 | 60.28] loss=2.78 avg=3.20\n",
            "[113 | 60.72] loss=3.10 avg=3.20\n",
            "[114 | 61.15] loss=2.89 avg=3.20\n",
            "[115 | 61.59] loss=2.97 avg=3.19\n",
            "[116 | 62.03] loss=3.03 avg=3.19\n",
            "[117 | 62.47] loss=3.34 avg=3.19\n",
            "[118 | 62.90] loss=3.14 avg=3.19\n",
            "[119 | 63.34] loss=3.10 avg=3.19\n",
            "[120 | 63.78] loss=3.15 avg=3.19\n",
            "[121 | 64.22] loss=3.16 avg=3.19\n",
            "[122 | 64.66] loss=2.97 avg=3.19\n",
            "[123 | 65.10] loss=3.03 avg=3.18\n",
            "[124 | 65.54] loss=2.99 avg=3.18\n",
            "[125 | 65.98] loss=3.04 avg=3.18\n",
            "[126 | 66.42] loss=2.96 avg=3.18\n",
            "[127 | 66.86] loss=3.16 avg=3.18\n",
            "[128 | 67.31] loss=3.14 avg=3.18\n",
            "[129 | 67.75] loss=3.10 avg=3.18\n",
            "[130 | 68.19] loss=3.01 avg=3.17\n",
            "[131 | 68.63] loss=3.19 avg=3.17\n",
            "[132 | 69.07] loss=2.90 avg=3.17\n",
            "[133 | 69.51] loss=2.88 avg=3.17\n",
            "[134 | 69.95] loss=3.28 avg=3.17\n",
            "[135 | 70.39] loss=2.98 avg=3.16\n",
            "[136 | 70.84] loss=3.08 avg=3.16\n",
            "[137 | 71.28] loss=3.16 avg=3.16\n",
            "[138 | 71.72] loss=3.13 avg=3.16\n",
            "[139 | 72.17] loss=2.98 avg=3.16\n",
            "[140 | 72.61] loss=3.20 avg=3.16\n",
            "[141 | 73.05] loss=2.87 avg=3.16\n",
            "[142 | 73.50] loss=3.10 avg=3.16\n",
            "[143 | 73.94] loss=3.13 avg=3.16\n",
            "[144 | 74.39] loss=3.12 avg=3.16\n",
            "[145 | 74.83] loss=3.11 avg=3.16\n",
            "[146 | 75.28] loss=3.40 avg=3.16\n",
            "[147 | 75.72] loss=3.12 avg=3.16\n",
            "[148 | 76.17] loss=3.02 avg=3.16\n",
            "[149 | 76.62] loss=2.87 avg=3.15\n",
            "[150 | 77.06] loss=2.71 avg=3.15\n",
            "[151 | 77.51] loss=2.92 avg=3.14\n",
            "[152 | 77.96] loss=2.92 avg=3.14\n",
            "[153 | 78.40] loss=2.98 avg=3.14\n",
            "[154 | 78.85] loss=2.89 avg=3.14\n",
            "[155 | 79.30] loss=3.29 avg=3.14\n",
            "[156 | 79.75] loss=3.09 avg=3.14\n",
            "[157 | 80.20] loss=2.76 avg=3.13\n",
            "[158 | 80.64] loss=2.88 avg=3.13\n",
            "[159 | 81.09] loss=2.89 avg=3.13\n",
            "[160 | 81.54] loss=3.01 avg=3.12\n",
            "[161 | 81.99] loss=3.09 avg=3.12\n",
            "[162 | 82.44] loss=2.87 avg=3.12\n",
            "[163 | 82.89] loss=3.02 avg=3.12\n",
            "[164 | 83.34] loss=3.16 avg=3.12\n",
            "[165 | 83.80] loss=3.05 avg=3.12\n",
            "[166 | 84.25] loss=2.83 avg=3.12\n",
            "[167 | 84.70] loss=3.09 avg=3.12\n",
            "[168 | 85.16] loss=2.82 avg=3.11\n",
            "[169 | 85.61] loss=2.86 avg=3.11\n",
            "[170 | 86.06] loss=2.67 avg=3.10\n",
            "[171 | 86.51] loss=2.72 avg=3.10\n",
            "[172 | 86.97] loss=2.90 avg=3.10\n",
            "[173 | 87.42] loss=2.90 avg=3.09\n",
            "[174 | 87.88] loss=2.83 avg=3.09\n",
            "[175 | 88.34] loss=2.99 avg=3.09\n",
            "[176 | 88.79] loss=2.88 avg=3.09\n",
            "[177 | 89.25] loss=2.89 avg=3.08\n",
            "[178 | 89.71] loss=2.95 avg=3.08\n",
            "[179 | 90.17] loss=3.38 avg=3.09\n",
            "[180 | 90.62] loss=3.22 avg=3.09\n",
            "[181 | 91.08] loss=3.03 avg=3.09\n",
            "[182 | 91.54] loss=2.86 avg=3.08\n",
            "[183 | 92.00] loss=2.92 avg=3.08\n",
            "[184 | 92.46] loss=2.78 avg=3.08\n",
            "[185 | 92.93] loss=2.92 avg=3.08\n",
            "[186 | 93.39] loss=2.57 avg=3.07\n",
            "[187 | 93.85] loss=2.83 avg=3.07\n",
            "[188 | 94.31] loss=3.33 avg=3.07\n",
            "[189 | 94.78] loss=3.03 avg=3.07\n",
            "[190 | 95.24] loss=2.93 avg=3.07\n",
            "[191 | 95.70] loss=3.02 avg=3.07\n",
            "[192 | 96.16] loss=3.21 avg=3.07\n",
            "[193 | 96.63] loss=2.71 avg=3.07\n",
            "[194 | 97.10] loss=3.16 avg=3.07\n",
            "[195 | 97.56] loss=2.92 avg=3.07\n",
            "[196 | 98.04] loss=3.05 avg=3.07\n",
            "[197 | 98.50] loss=2.93 avg=3.06\n",
            "[198 | 98.97] loss=2.98 avg=3.06\n",
            "[199 | 99.44] loss=2.92 avg=3.06\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "This year, one could just as easily have been the first to get together under the guise of some special activity. If this year's affair had been undertaken in the same way, the other would have been far deeper. We are not sure if the matter was not, for instance, a series of affairs of the first kind, or whether the whole was a very simple deal of play, and the first act was intended to convey to the other a strong impression of the desire among the members, and to bring them to work.\n",
            "\n",
            "Our impression, therefore, that most of the members of the first group had played together so far as was necessary, or what had begun as an effort had continued with the next, would probably have remained the same.\n",
            "\n",
            "It would seem to me highly improbable that anything but such an effort was to be found in any of the present or future groups of members. There appears a special significance attached to the fact that all those who have been subjected to the pressure of the most intense and pressing desire--especially those capable of forming and maintaining sexual relation--have in all probability already had the foresight to develop sexual feelings in the course of many years, which made all the possible changes possible in their character, and to continue to develop this feeling for many years to come. We can be sure that they had not been made aware of these emotions, or at all who had not been capable to develop them; and all those who have been able to develop these feelings themselves have made themselves indispensable. Their development had to begin according to the first of these rules, and at the same time according to the second. Whatever the extent of their ability, however, there is no reason why they should be content or willing to make themselves indispensable; they can just as easily develop them in a manner in which no one has ever been able to maintain, which makes the whole problem of the first matter entirely superfluous. For the latter matter every one is capable of. What is important is the first of these rules, and we are to accept that the problem of the third act should be solved in such a manner that the first group of members will be incapable of making any changes to themselves, and that the problem of the fourth ought to be solved with all its elements.\n",
            "\n",
            "When we have looked more closely at the cases which have been so far proved in order to demonstrate that the expression of a special emotion is due to several motives or motives (a term I have just described as an explanation of a certain matter of fact), we find even more that the emotion is of the very essence. The motive is one of desire. The motive was a wish to keep from giving birth, but there will be no birth now to the womb, and that wish is satisfied before the child has been born; but this wish had been satisfied during childhood, and not during puberty. The first motive of the present situation is a wish to avoid getting pregnant, and the first motive of the future situation is a wish to keep the child's birth to himself. As a rule, the expression of a special longing in some one in regard to the birth of the child, can be represented as an excited desire. The expression of another special longing in the same regard with affection or love for the child, or even for a special longing for the child in regard to the child himself, can be represented as excited, as it is in the expression of a special desire.\n",
            "\n",
            "We have already described some of the emotions of the last years, the first group of members of the second group, and the situation in the middle of the year when the persons who had been subjected to the most intense, and who could only now have formed and maintained sexual relations, must now have been exposed to the greatest change in their character as compared with their relations of the preceding year.\n",
            "\n",
            "On the one hand, we can easily see a great change in the character, qualities, and character of the persons in the previous years who had been subject to the exertion of the greatest change in their character and qualities in those persons in the present year, but can also show that no changes had taken place at all, and that no such changes as have already appeared occurable in all persons at that time. On the other hand, there are still some persons who showed the least signs of being changed, and all the persons who had been subjected to the greatest changes were changed from persons who had been treated much more poorly than they were now. From the very first persons who had been subjected to some of the same treatment as now, some of them had already expressed some of this wish, but without giving up the idea. Such persons in the preceding year who had been subjected to a great regression have also been changed; such persons who had been not subjected to the same treatment, still remain the same persons.\n",
            "\n",
            "From all this it is impossible to say anything about any one of the changes. A great number of persons, for instance, who are now in their thirties, are now not in themselves of\n",
            "\n",
            "[200 | 111.54] loss=2.95 avg=3.06\n",
            "[201 | 112.03] loss=2.93 avg=3.06\n",
            "[202 | 112.52] loss=2.75 avg=3.06\n",
            "[203 | 113.00] loss=2.75 avg=3.05\n",
            "[204 | 113.49] loss=3.08 avg=3.05\n",
            "[205 | 113.98] loss=2.68 avg=3.05\n",
            "[206 | 114.47] loss=2.96 avg=3.05\n",
            "[207 | 114.95] loss=2.96 avg=3.05\n",
            "[208 | 115.44] loss=3.17 avg=3.05\n",
            "[209 | 115.93] loss=2.91 avg=3.05\n",
            "[210 | 116.42] loss=2.98 avg=3.04\n",
            "[211 | 116.90] loss=2.91 avg=3.04\n",
            "[212 | 117.39] loss=2.91 avg=3.04\n",
            "[213 | 117.88] loss=3.04 avg=3.04\n",
            "[214 | 118.36] loss=2.93 avg=3.04\n",
            "[215 | 118.85] loss=2.61 avg=3.04\n",
            "[216 | 119.34] loss=2.86 avg=3.03\n",
            "[217 | 119.83] loss=2.80 avg=3.03\n",
            "[218 | 120.31] loss=2.85 avg=3.03\n",
            "[219 | 120.80] loss=2.83 avg=3.03\n",
            "[220 | 121.28] loss=3.18 avg=3.03\n",
            "[221 | 121.77] loss=2.42 avg=3.02\n",
            "[222 | 122.24] loss=3.01 avg=3.02\n",
            "[223 | 122.73] loss=2.81 avg=3.02\n",
            "[224 | 123.20] loss=2.69 avg=3.02\n",
            "[225 | 123.68] loss=2.72 avg=3.01\n",
            "[226 | 124.17] loss=2.90 avg=3.01\n",
            "[227 | 124.64] loss=2.86 avg=3.01\n",
            "[228 | 125.12] loss=2.87 avg=3.01\n",
            "[229 | 125.59] loss=2.79 avg=3.01\n",
            "[230 | 126.07] loss=2.92 avg=3.00\n",
            "[231 | 126.54] loss=2.79 avg=3.00\n",
            "[232 | 127.02] loss=2.53 avg=3.00\n",
            "[233 | 127.49] loss=3.02 avg=3.00\n",
            "[234 | 127.96] loss=2.80 avg=2.99\n",
            "[235 | 128.43] loss=2.80 avg=2.99\n",
            "[236 | 128.90] loss=2.76 avg=2.99\n",
            "[237 | 129.37] loss=3.01 avg=2.99\n",
            "[238 | 129.84] loss=2.76 avg=2.99\n",
            "[239 | 130.31] loss=2.83 avg=2.99\n",
            "[240 | 130.77] loss=2.52 avg=2.98\n",
            "[241 | 131.24] loss=2.92 avg=2.98\n",
            "[242 | 131.70] loss=2.55 avg=2.98\n",
            "[243 | 132.17] loss=2.76 avg=2.97\n",
            "[244 | 132.64] loss=2.64 avg=2.97\n",
            "[245 | 133.11] loss=2.94 avg=2.97\n",
            "[246 | 133.58] loss=2.68 avg=2.97\n",
            "[247 | 134.04] loss=2.92 avg=2.97\n",
            "[248 | 134.51] loss=3.09 avg=2.97\n",
            "[249 | 134.98] loss=2.99 avg=2.97\n",
            "[250 | 135.45] loss=2.51 avg=2.96\n",
            "[251 | 135.92] loss=2.84 avg=2.96\n",
            "[252 | 136.38] loss=2.80 avg=2.96\n",
            "[253 | 136.85] loss=2.96 avg=2.96\n",
            "[254 | 137.32] loss=2.63 avg=2.96\n",
            "[255 | 137.78] loss=2.64 avg=2.95\n",
            "[256 | 138.25] loss=2.73 avg=2.95\n",
            "[257 | 138.72] loss=2.77 avg=2.95\n",
            "[258 | 139.19] loss=2.69 avg=2.94\n",
            "[259 | 139.65] loss=2.75 avg=2.94\n",
            "[260 | 140.12] loss=2.93 avg=2.94\n",
            "[261 | 140.58] loss=2.76 avg=2.94\n",
            "[262 | 141.05] loss=3.01 avg=2.94\n",
            "[263 | 141.51] loss=3.02 avg=2.94\n",
            "[264 | 141.97] loss=2.62 avg=2.94\n",
            "[265 | 142.44] loss=2.89 avg=2.94\n",
            "[266 | 142.90] loss=2.88 avg=2.94\n",
            "[267 | 143.37] loss=2.86 avg=2.94\n",
            "[268 | 143.83] loss=2.85 avg=2.94\n",
            "[269 | 144.29] loss=2.73 avg=2.93\n",
            "[270 | 144.76] loss=2.90 avg=2.93\n",
            "[271 | 145.22] loss=2.75 avg=2.93\n",
            "[272 | 145.69] loss=2.94 avg=2.93\n",
            "[273 | 146.15] loss=2.87 avg=2.93\n",
            "[274 | 146.61] loss=2.87 avg=2.93\n",
            "[275 | 147.07] loss=2.95 avg=2.93\n",
            "[276 | 147.53] loss=2.85 avg=2.93\n",
            "[277 | 148.00] loss=2.54 avg=2.93\n",
            "[278 | 148.46] loss=2.66 avg=2.92\n",
            "[279 | 148.92] loss=2.60 avg=2.92\n",
            "[280 | 149.38] loss=2.43 avg=2.91\n",
            "[281 | 149.84] loss=2.87 avg=2.91\n",
            "[282 | 150.30] loss=2.48 avg=2.91\n",
            "[283 | 150.76] loss=2.86 avg=2.91\n",
            "[284 | 151.23] loss=3.06 avg=2.91\n",
            "[285 | 151.68] loss=3.03 avg=2.91\n",
            "[286 | 152.15] loss=2.87 avg=2.91\n",
            "[287 | 152.61] loss=2.68 avg=2.91\n",
            "[288 | 153.07] loss=2.80 avg=2.91\n",
            "[289 | 153.53] loss=2.72 avg=2.91\n",
            "[290 | 153.99] loss=3.06 avg=2.91\n",
            "[291 | 154.46] loss=2.51 avg=2.90\n",
            "[292 | 154.92] loss=2.63 avg=2.90\n",
            "[293 | 155.38] loss=2.90 avg=2.90\n",
            "[294 | 155.84] loss=2.74 avg=2.90\n",
            "[295 | 156.30] loss=2.99 avg=2.90\n",
            "[296 | 156.76] loss=2.79 avg=2.90\n",
            "[297 | 157.22] loss=2.69 avg=2.90\n",
            "[298 | 157.69] loss=2.50 avg=2.89\n",
            "[299 | 158.15] loss=2.55 avg=2.89\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " kindness for others. We have done everything we can to keep the peace between us. That we do not take refuge in the one we love and treat contemptuously upon those who oppose us, is a fact to which we must confess as well as to which we must also admit the reality of an intermingled relationship. It is, therefore, not an act of love, but the rejection of an antagonistic one which the two meet on the road to their separation; to do away with it in favor of an intermingled one. We must, however, avoid this distortion by keeping in mind that no one else can take refuge in the one we esteem but himself. Thus the poet, like every other man among his fellow beings, would not allow that love should be avoided if at the same time it should be maintained. It must be maintained only if in the course of an action it can be made clear that, as it were, it can not be maintained in some other way as well (but if this is so, the point was left the same) . I would not like to be said here to be in a hurry, for I do not believe that I would have the will to say what I do here in the shortest possible time if I was able to get a very brief and clear description of what the poet says. I think even a simple description of something that has already received a certain regard from one of the interpreters will probably suffice for my purposes; I do not think I will have sufficient assurance from my native countryman, the translator of the works, that I are in a hurry to get him to do something for me. In like manner, it is a matter for us all, a matter of keeping up a great deal of trust in the reader, which also demands an intermingled and interwoven relation between an object and of itself. The translator must then be forced to take in account the meaning and motives of the two thoughts which constitute a kind of harmony. They will never be recognized by us as a separate thought and of themselves as a united one, but we can at least in a certain way be certain that there will always be a difference (a difference in the meaning) as to the two thoughts. No, on the contrary, the translator does not take care if the one thoughts are found to be the unity of the other; he acts, indeed, as one unity over and above it, but never as two or more distinct thoughts. Hence the connection with other points in his analysis, in all his work. I am not saying, therefore, that the translator is in a hurry, or that it is expedient that he should get his wish granted as long as every effort should be had to meet the task. I merely wish that this time may be made just as expedient for those who are prepared to do so. The difficulty which it causes me to raise to this point, however, is only that which must be done before the work can continue. I am willing to admit a little anxiety regarding this. My position is merely that one of my interpreters has sent me an invitation to be in a hurry, and for the whole of the year to be in such an hurry that the work's interpretation cannot possibly cope with that. The work is then to be done and the interpreters themselves have to do all that would be asked of them by their work. The point does not need this consideration, however, that the work is to be fixed in such a state. In the same way that some one wishes to go to the theater without a ticket, I myself may wish to go, or at a very low interest, without any ticket, but I suspect at this moment that an express ticket, without such a ticket, would be an easy one. I will, however, not, of necessity, allow myself to be taken advantage of, because, on the contrary, the work is to be in a state of a halt. All the work is to proceed in the direction of the intended work, and I am, as we have remarked, prepared for that. So far as my position is concerned, I wish to have a good deal of confidence that nothing can be said to compromise my position, for I do not fear myself to go. The dream seems to be of myself, after all, at the same time at liberty to dream for himself. On the other hand, it may appear that I am indeed at the same time the person who has now to be replaced. At the same time I wish that I may have the same hope as myself for other persons who find themselves out of position even before the work is on their side. It is probably better not indeed to trust yourself, but to keep at rest and to keep at some distance at some distance where the time for the dream will be least inconvenienced, where this will affect the character of the work as well as the other thoughts; and as far as I can possibly estimate, this will mainly depend upon my understanding the dream as it existed before this work came to\n",
            "\n",
            "[300 | 170.43] loss=2.64 avg=2.89\n",
            "[301 | 170.89] loss=2.72 avg=2.88\n",
            "[302 | 171.36] loss=2.73 avg=2.88\n",
            "[303 | 171.83] loss=2.80 avg=2.88\n",
            "[304 | 172.29] loss=2.93 avg=2.88\n",
            "[305 | 172.76] loss=2.93 avg=2.88\n",
            "[306 | 173.22] loss=2.63 avg=2.88\n",
            "[307 | 173.69] loss=2.40 avg=2.87\n",
            "[308 | 174.15] loss=2.53 avg=2.87\n",
            "[309 | 174.62] loss=2.76 avg=2.87\n",
            "[310 | 175.08] loss=2.65 avg=2.87\n",
            "[311 | 175.54] loss=2.85 avg=2.87\n",
            "[312 | 176.01] loss=2.81 avg=2.87\n",
            "[313 | 176.48] loss=2.68 avg=2.86\n",
            "[314 | 176.95] loss=2.57 avg=2.86\n",
            "[315 | 177.41] loss=2.79 avg=2.86\n",
            "[316 | 177.88] loss=2.59 avg=2.86\n",
            "[317 | 178.34] loss=2.57 avg=2.85\n",
            "[318 | 178.81] loss=2.63 avg=2.85\n",
            "[319 | 179.27] loss=2.73 avg=2.85\n",
            "[320 | 179.75] loss=2.49 avg=2.85\n",
            "[321 | 180.21] loss=2.64 avg=2.85\n",
            "[322 | 180.68] loss=2.90 avg=2.85\n",
            "[323 | 181.15] loss=2.67 avg=2.84\n",
            "[324 | 181.61] loss=2.72 avg=2.84\n",
            "[325 | 182.08] loss=2.98 avg=2.84\n",
            "[326 | 182.55] loss=2.74 avg=2.84\n",
            "[327 | 183.02] loss=2.47 avg=2.84\n",
            "[328 | 183.48] loss=2.36 avg=2.83\n",
            "[329 | 183.95] loss=2.45 avg=2.83\n",
            "[330 | 184.42] loss=3.07 avg=2.83\n",
            "[331 | 184.88] loss=2.53 avg=2.83\n",
            "[332 | 185.35] loss=2.59 avg=2.83\n",
            "[333 | 185.82] loss=2.67 avg=2.83\n",
            "[334 | 186.29] loss=2.63 avg=2.82\n",
            "[335 | 186.75] loss=2.97 avg=2.83\n",
            "[336 | 187.22] loss=2.55 avg=2.82\n",
            "[337 | 187.69] loss=2.65 avg=2.82\n",
            "[338 | 188.15] loss=2.60 avg=2.82\n",
            "[339 | 188.62] loss=2.28 avg=2.81\n",
            "[340 | 189.09] loss=2.59 avg=2.81\n",
            "[341 | 189.55] loss=2.82 avg=2.81\n",
            "[342 | 190.02] loss=2.79 avg=2.81\n",
            "[343 | 190.49] loss=2.60 avg=2.81\n",
            "[344 | 190.95] loss=2.82 avg=2.81\n",
            "[345 | 191.42] loss=2.83 avg=2.81\n",
            "[346 | 191.89] loss=3.07 avg=2.81\n",
            "[347 | 192.35] loss=3.04 avg=2.81\n",
            "[348 | 192.82] loss=2.44 avg=2.81\n",
            "[349 | 193.29] loss=2.87 avg=2.81\n",
            "[350 | 193.76] loss=2.80 avg=2.81\n",
            "[351 | 194.22] loss=2.21 avg=2.80\n",
            "[352 | 194.69] loss=2.53 avg=2.80\n",
            "[353 | 195.16] loss=2.64 avg=2.80\n",
            "[354 | 195.62] loss=3.10 avg=2.80\n",
            "[355 | 196.09] loss=2.60 avg=2.80\n",
            "[356 | 196.56] loss=2.54 avg=2.80\n",
            "[357 | 197.03] loss=2.42 avg=2.79\n",
            "[358 | 197.49] loss=2.53 avg=2.79\n",
            "[359 | 197.96] loss=2.52 avg=2.79\n",
            "[360 | 198.43] loss=2.54 avg=2.79\n",
            "[361 | 198.90] loss=2.69 avg=2.78\n",
            "[362 | 199.37] loss=2.65 avg=2.78\n",
            "[363 | 199.83] loss=2.78 avg=2.78\n",
            "[364 | 200.30] loss=2.57 avg=2.78\n",
            "[365 | 200.77] loss=2.41 avg=2.78\n",
            "[366 | 201.24] loss=2.81 avg=2.78\n",
            "[367 | 201.71] loss=2.80 avg=2.78\n",
            "[368 | 202.17] loss=2.56 avg=2.78\n",
            "[369 | 202.64] loss=2.72 avg=2.78\n",
            "[370 | 203.11] loss=2.80 avg=2.78\n",
            "[371 | 203.58] loss=2.66 avg=2.77\n",
            "[372 | 204.05] loss=2.61 avg=2.77\n",
            "[373 | 204.51] loss=2.84 avg=2.77\n",
            "[374 | 204.98] loss=2.78 avg=2.77\n",
            "[375 | 205.45] loss=2.59 avg=2.77\n",
            "[376 | 205.92] loss=2.51 avg=2.77\n",
            "[377 | 206.39] loss=2.47 avg=2.77\n",
            "[378 | 206.85] loss=2.54 avg=2.76\n",
            "[379 | 207.32] loss=2.64 avg=2.76\n",
            "[380 | 207.79] loss=2.66 avg=2.76\n",
            "[381 | 208.26] loss=2.44 avg=2.76\n",
            "[382 | 208.73] loss=2.70 avg=2.76\n",
            "[383 | 209.19] loss=2.96 avg=2.76\n",
            "[384 | 209.66] loss=2.39 avg=2.76\n",
            "[385 | 210.13] loss=2.43 avg=2.75\n",
            "[386 | 210.60] loss=2.44 avg=2.75\n",
            "[387 | 211.07] loss=2.42 avg=2.75\n",
            "[388 | 211.54] loss=2.41 avg=2.74\n",
            "[389 | 212.01] loss=2.58 avg=2.74\n",
            "[390 | 212.47] loss=2.74 avg=2.74\n",
            "[391 | 212.94] loss=2.47 avg=2.74\n",
            "[392 | 213.41] loss=2.47 avg=2.73\n",
            "[393 | 213.88] loss=2.72 avg=2.73\n",
            "[394 | 214.35] loss=2.77 avg=2.73\n",
            "[395 | 214.81] loss=2.64 avg=2.73\n",
            "[396 | 215.28] loss=2.53 avg=2.73\n",
            "[397 | 215.75] loss=3.00 avg=2.73\n",
            "[398 | 216.22] loss=2.75 avg=2.73\n",
            "[399 | 216.68] loss=2.50 avg=2.73\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " other that the author would be far too patient or anxious if, with the aid of a small apparatus, he were left behind with a figure which looked as though he was walking in the dark, and we can expect that the author would be far too patient or anxious if, with a small apparatus, he were left behind with a figure which looked like a walking corpse. On any other account it must be far off, as the image of a walking corpse certainly is; no other image can seem more striking to my fancy. As for the fact of the hallucination of the dreaming person, I take it that the same thing may also be said respecting the representation of the waking state.\n",
            "\n",
            "[1] — A very strange one, to be sure.\n",
            "\n",
            "\n",
            "[2] — _I_ would_ prefer inasmuch as we cannot quite trust our conclusions as to the psychical content of an event; as soon as one puts forward a possibility, as in this connection, one becomes unable to find a way in, but becomes forced to accept it. We are thus in a state of denial, as, \"That's all, just as a scientist can't prove a hypothesis. That's just a trick. We can't say anything really without contradiction; it can't be done.\" [3] — _I_ wish I had written this _instead of _a_ like-minded young man_.\n",
            "\n",
            "\n",
            "[4] There are many passages in the original. For instance, the meaning of \n",
            "\n",
            ": \"If you had died,\", _\n",
            "\n",
            "is \"If I had not died,\" _\n",
            "\n",
            "is \"If I had been unable to.\" _\n",
            "\n",
            "The interpretation: \"If I had not been unable to,\" is really \"If I would have been unable to,\" and probably the \n",
            "same interpretation also applies to all the other passages in this passage, viz. _\n",
            "\n",
            "\n",
            "THE CONSCIOUSNESS : \"If you had died, everything would have been a dream.\" [1]\n",
            "\n",
            "\n",
            "\n",
            "The meaning of the following words in this passage is quite typical: \"If I had not died.\" _\n",
            "\n",
            "The interpretation: \"There is nothing special to this, viz. there is no dream in the dream. Thus the dream is just an event.\" [2]\n",
            "\n",
            "\n",
            "[5] _I_ had made it to sleep. _\n",
            "\n",
            "The interpretation: _I_ had left the room._ .\n",
            "\n",
            "The interpretation: \"Every once in awhile there arose a dream in the room to which I had always been subjected.\" [3]\n",
            "\n",
            "\n",
            "CONCLUSION : The authors seem to have been dealing with quite a common mental issue, inasmuch as it seems to me probable that a \"mental state\" is often the same for all dreamers as it is for those who dream with only a few persons. This is because we have been misled by the writers, on account of their ignorance of the psychic history of the individual person, into thinking that the dream is the same as if there were only one person sleeping. These erroneous views, however, prove the falsity of the dream dream, that is, their distortion of reality. [4] A recent study found that the dream dreams of young people who had only a few persons at a time sleep, and that these sleeping persons may never find another way of sleeping. It is not impossible that the sleep of the sleeping persons becomes paralyzed, and that, at the same time, the waking person becomes paralyzed, to the advantage of the sleeping person who is sleeping and who is not conscious. The dream dreams appear also to be based upon the very notion of sleep, viz. the relation of the dream to sleep. We can thus say, for this reason alone—there is nothing special to the dream dream . What dreams are those attributed to sleeping? It will be somewhat true that a number have been attributed to dreaming in order to confirm the proposition that certain qualities of the character of the dreaming person, as contrasted to that of the waking person, are not so well recognized. What are the other qualities of the character of the sleeping person, which have not been revealed to us in the dream dreams—their psychic states, their peculiar intensity, or some other characteristic? For we are concerned here with those features which, if confirmed, form a factor in the psychical character of the patient, which we should not conceal when talking of these sleeping qualities. These features are not always as obvious as are the other qualities which are mentioned, but they are always found. What can be the significance of every such occurrence? The dream dreams are very obviously a dream of psychical origin, with the following features; —they may also be compared with other mental phenomena such as nightmares, the feeling of pain, the recollection of dreams, as well as dreamers who use drugs in the dream state. The dream state is often very complicated. In the same way as in dreams, in which the dream goes on, the psychic activities are also changed. We find that the dream\n",
            "\n",
            "[400 | 228.70] loss=2.30 avg=2.73\n",
            "[401 | 229.17] loss=2.64 avg=2.73\n",
            "[402 | 229.64] loss=2.98 avg=2.73\n",
            "[403 | 230.11] loss=2.80 avg=2.73\n",
            "[404 | 230.58] loss=2.77 avg=2.73\n",
            "[405 | 231.05] loss=2.52 avg=2.73\n",
            "[406 | 231.52] loss=2.40 avg=2.73\n",
            "[407 | 231.99] loss=2.32 avg=2.72\n",
            "[408 | 232.45] loss=2.64 avg=2.72\n",
            "[409 | 232.93] loss=2.52 avg=2.72\n",
            "[410 | 233.40] loss=2.28 avg=2.71\n",
            "[411 | 233.87] loss=2.51 avg=2.71\n",
            "[412 | 234.34] loss=2.47 avg=2.71\n",
            "[413 | 234.81] loss=2.79 avg=2.71\n",
            "[414 | 235.27] loss=2.47 avg=2.71\n",
            "[415 | 235.74] loss=2.54 avg=2.71\n",
            "[416 | 236.21] loss=2.41 avg=2.70\n",
            "[417 | 236.68] loss=2.62 avg=2.70\n",
            "[418 | 237.14] loss=2.37 avg=2.70\n",
            "[419 | 237.61] loss=2.40 avg=2.70\n",
            "[420 | 238.08] loss=2.48 avg=2.69\n",
            "[421 | 238.55] loss=2.67 avg=2.69\n",
            "[422 | 239.01] loss=2.08 avg=2.69\n",
            "[423 | 239.48] loss=2.85 avg=2.69\n",
            "[424 | 239.95] loss=2.87 avg=2.69\n",
            "[425 | 240.42] loss=2.42 avg=2.69\n",
            "[426 | 240.88] loss=2.50 avg=2.69\n",
            "[427 | 241.35] loss=2.30 avg=2.68\n",
            "[428 | 241.82] loss=2.79 avg=2.68\n",
            "[429 | 242.29] loss=2.49 avg=2.68\n",
            "[430 | 242.75] loss=2.52 avg=2.68\n",
            "[431 | 243.22] loss=2.79 avg=2.68\n",
            "[432 | 243.69] loss=2.42 avg=2.68\n",
            "[433 | 244.16] loss=2.28 avg=2.67\n",
            "[434 | 244.63] loss=2.51 avg=2.67\n",
            "[435 | 245.10] loss=2.41 avg=2.67\n",
            "[436 | 245.57] loss=2.45 avg=2.67\n",
            "[437 | 246.04] loss=2.38 avg=2.66\n",
            "[438 | 246.51] loss=2.37 avg=2.66\n",
            "[439 | 246.98] loss=3.10 avg=2.67\n",
            "[440 | 247.45] loss=2.52 avg=2.66\n",
            "[441 | 247.91] loss=2.36 avg=2.66\n",
            "[442 | 248.38] loss=2.27 avg=2.66\n",
            "[443 | 248.85] loss=2.11 avg=2.65\n",
            "[444 | 249.32] loss=2.33 avg=2.65\n",
            "[445 | 249.78] loss=2.47 avg=2.65\n",
            "[446 | 250.25] loss=2.41 avg=2.64\n",
            "[447 | 250.72] loss=2.42 avg=2.64\n",
            "[448 | 251.19] loss=2.50 avg=2.64\n",
            "[449 | 251.65] loss=2.69 avg=2.64\n",
            "[450 | 252.12] loss=2.88 avg=2.64\n",
            "[451 | 252.59] loss=2.24 avg=2.64\n",
            "[452 | 253.06] loss=2.20 avg=2.64\n",
            "[453 | 253.52] loss=2.25 avg=2.63\n",
            "[454 | 253.99] loss=2.34 avg=2.63\n",
            "[455 | 254.46] loss=2.30 avg=2.62\n",
            "[456 | 254.92] loss=2.55 avg=2.62\n",
            "[457 | 255.39] loss=2.38 avg=2.62\n",
            "[458 | 255.86] loss=2.45 avg=2.62\n",
            "[459 | 256.33] loss=2.57 avg=2.62\n",
            "[460 | 256.79] loss=2.39 avg=2.62\n",
            "[461 | 257.26] loss=2.47 avg=2.62\n",
            "[462 | 257.73] loss=2.54 avg=2.61\n",
            "[463 | 258.20] loss=2.32 avg=2.61\n",
            "[464 | 258.67] loss=2.18 avg=2.61\n",
            "[465 | 259.13] loss=2.70 avg=2.61\n",
            "[466 | 259.60] loss=2.11 avg=2.60\n",
            "[467 | 260.07] loss=2.05 avg=2.60\n",
            "[468 | 260.53] loss=2.34 avg=2.60\n",
            "[469 | 261.00] loss=2.53 avg=2.59\n",
            "[470 | 261.47] loss=2.03 avg=2.59\n",
            "[471 | 261.93] loss=2.76 avg=2.59\n",
            "[472 | 262.40] loss=2.30 avg=2.59\n",
            "[473 | 262.87] loss=2.40 avg=2.59\n",
            "[474 | 263.34] loss=2.35 avg=2.58\n",
            "[475 | 263.80] loss=2.37 avg=2.58\n",
            "[476 | 264.27] loss=2.55 avg=2.58\n",
            "[477 | 264.74] loss=2.38 avg=2.58\n",
            "[478 | 265.21] loss=2.15 avg=2.57\n",
            "[479 | 265.68] loss=2.35 avg=2.57\n",
            "[480 | 266.14] loss=2.37 avg=2.57\n",
            "[481 | 266.61] loss=2.70 avg=2.57\n",
            "[482 | 267.08] loss=2.37 avg=2.57\n",
            "[483 | 267.55] loss=2.48 avg=2.57\n",
            "[484 | 268.02] loss=2.03 avg=2.56\n",
            "[485 | 268.48] loss=2.61 avg=2.56\n",
            "[486 | 268.95] loss=2.86 avg=2.57\n",
            "[487 | 269.41] loss=2.74 avg=2.57\n",
            "[488 | 269.88] loss=2.36 avg=2.57\n",
            "[489 | 270.35] loss=2.32 avg=2.56\n",
            "[490 | 270.82] loss=2.80 avg=2.57\n",
            "[491 | 271.29] loss=1.86 avg=2.56\n",
            "[492 | 271.76] loss=2.33 avg=2.56\n",
            "[493 | 272.22] loss=2.90 avg=2.56\n",
            "[494 | 272.69] loss=2.62 avg=2.56\n",
            "[495 | 273.16] loss=2.44 avg=2.56\n",
            "[496 | 273.63] loss=2.33 avg=2.56\n",
            "[497 | 274.09] loss=2.19 avg=2.55\n",
            "[498 | 274.56] loss=2.32 avg=2.55\n",
            "[499 | 275.03] loss=2.49 avg=2.55\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the that they may appear to be more real than what they once were, or may render little of real service. But the objection does not succeed in the formation of any opinion, but is left unaddressed. I am contented with expressing it by saying that if nothing is more pleasing to me than being to be loved, if nothing becomes more harmful to me than the perception of some unpleasant object, nothing is happier than being loved. In the latter case the real inconveniences may not be found, but the symptoms. I say, there is nothing more harmful than an unpleasant feeling, as well as an unpleasant object. The motive power to effect one thing by another is likewise in such cases the same; that what is most pleasing to me is the desire of the mind is most harmful to the mind. Such instances are as absurd as the absurdity of the saying, \"In the beginning there was no such thing as an equal.\" Their real meaning lies in the difference in the motive power to effect something by its inconveniences, but the greater. The relation is not the same; it is a question of the principle of succession, between the inconveniences, and the real inconveniences, which are in reality quite the same. They are the relation between an event and the wish. They do, indeed, exist, but they are quite useless. We must leave behind them the wish that every individual event does as it was intended. We cannot be sure of a good motive power in any event which creates neither pleasure nor pain; we have to assume that every effort to make such an event pleasure or pain must be expended on those inconveniences which do not represent any pleasure.\n",
            "\n",
            "Another objection has arisen from those who regard the desire as real, and the wish as a dream thought, which furnishes a convenient example; there have also been reports of young persons falling into this latter mode of life. A boy who has once received a fine for being a strict dogman, and has made his own way according to his wonts, finds himself lying on his back upon the grass-heavily ground, which was originally in some kind of a defensive position for his protection, whilst he was sleeping. The little girl is in a profound dream, not wanting to be in a position of any kind for sleep, and he is forced down one of the paces of sleeping to one of the tracks which leads downwards through the grass-heavily ground. The child was once invited to give a performance at a ball, to which both children were, indeed, in a position of their utmost importance, and which had just attained its end of watching. He asked the theater-goer what an honor it was for a child, when, on the track, he could just barely keep his head above the surface of the water. His performance was of trivial triviality. \"That's too little,\" he said to himself, \"how could it be that one can see without being in a position to see?\" It was a dream, indeed. In the dream a man is invited to become so-called by virtue of his occupation that he is known generally as \"the big man.\" The theater-goer, on the other hand, was invited to become king. The young man had become the king of the country, and had been the emperor, the \"big man.\" As the big man is called a king, he is known to be a king of small country; and, as it were, it was the prince himself, not the big man who had been crowned. To become a king you must always be under the same guardian as the king--in every case there is a guardian whose work it will be seen how he does it. So far our conception has now been replaced by our theory, in what follows; there are a number of cases of the same sort which are very well known in their day; in all these cases the real or actual guardian was at fault; the real guardian, or the one who should be so, is invariably found to have been so.\n",
            "\n",
            "The trouble arises when we say that there are others who, as parents, should have been to the knowledge of the day the wish of the child was to become a princess. It was not a wish of the day; all that the dream could remember of was that, whilst she was at bed, she had to run up and down, and pass at the foot of a mountain, but that she had to stand upon a small platform which was only a few inches high, which she used as a platform for climbing. She is at once attacked by three big trees, which then meet her on either side of which she may run upwards or downwards. She is attacked at once by several smaller trees which are not at all at her side; she is then able to make her way through the grassy parts of the platform to a small ledge, which, to be sure, she is at once enabled to run down. If her father, or her mother, had done the same, and had not forced\n",
            "\n",
            "[500 | 286.92] loss=2.23 avg=2.55\n",
            "[501 | 287.39] loss=2.33 avg=2.55\n",
            "[502 | 287.86] loss=2.44 avg=2.54\n",
            "[503 | 288.32] loss=2.79 avg=2.55\n",
            "[504 | 288.78] loss=1.96 avg=2.54\n",
            "[505 | 289.25] loss=3.12 avg=2.55\n",
            "[506 | 289.72] loss=2.42 avg=2.55\n",
            "[507 | 290.18] loss=2.21 avg=2.54\n",
            "[508 | 290.65] loss=2.13 avg=2.54\n",
            "[509 | 291.12] loss=2.14 avg=2.53\n",
            "[510 | 291.58] loss=2.21 avg=2.53\n",
            "[511 | 292.05] loss=2.75 avg=2.53\n",
            "[512 | 292.52] loss=2.13 avg=2.53\n",
            "[513 | 292.98] loss=2.33 avg=2.53\n",
            "[514 | 293.45] loss=2.21 avg=2.52\n",
            "[515 | 293.92] loss=2.20 avg=2.52\n",
            "[516 | 294.38] loss=2.62 avg=2.52\n",
            "[517 | 294.85] loss=1.91 avg=2.52\n",
            "[518 | 295.32] loss=2.06 avg=2.51\n",
            "[519 | 295.78] loss=2.14 avg=2.51\n",
            "[520 | 296.25] loss=2.46 avg=2.51\n",
            "[521 | 296.72] loss=2.15 avg=2.50\n",
            "[522 | 297.19] loss=2.41 avg=2.50\n",
            "[523 | 297.65] loss=2.30 avg=2.50\n",
            "[524 | 298.12] loss=2.42 avg=2.50\n",
            "[525 | 298.58] loss=2.22 avg=2.50\n",
            "[526 | 299.05] loss=2.22 avg=2.49\n",
            "[527 | 299.52] loss=2.38 avg=2.49\n",
            "[528 | 299.98] loss=2.22 avg=2.49\n",
            "[529 | 300.45] loss=2.40 avg=2.49\n",
            "[530 | 300.92] loss=2.55 avg=2.49\n",
            "[531 | 301.38] loss=2.55 avg=2.49\n",
            "[532 | 301.84] loss=2.49 avg=2.49\n",
            "[533 | 302.31] loss=2.16 avg=2.49\n",
            "[534 | 302.78] loss=2.53 avg=2.49\n",
            "[535 | 303.24] loss=2.47 avg=2.49\n",
            "[536 | 303.71] loss=2.20 avg=2.48\n",
            "[537 | 304.17] loss=2.16 avg=2.48\n",
            "[538 | 304.64] loss=2.27 avg=2.48\n",
            "[539 | 305.11] loss=2.21 avg=2.48\n",
            "[540 | 305.58] loss=2.47 avg=2.48\n",
            "[541 | 306.04] loss=2.43 avg=2.48\n",
            "[542 | 306.51] loss=2.05 avg=2.47\n",
            "[543 | 306.98] loss=2.45 avg=2.47\n",
            "[544 | 307.44] loss=2.18 avg=2.47\n",
            "[545 | 307.91] loss=2.96 avg=2.47\n",
            "[546 | 308.38] loss=2.06 avg=2.47\n",
            "[547 | 308.85] loss=2.77 avg=2.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1RJJDFOPnb"
      },
      "source": [
        "Save our checkpoints to start training again later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JretqG1zOXdi"
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-i7vERWbNS"
      },
      "source": [
        "Load your trained model for use in sampling below (117M or 345M)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeETvWvrbKga"
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np0r6qfXBeUX"
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmnSrXqtfRbq"
      },
      "source": [
        "Generate conditional samples from the model given a prompt you provide -  change top-k hyperparameter if desired (default is 40),  if you're using 345M, add \"--model-name 345M\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJj-iY4gHwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0998a2-e17f-48b1-eb74-607e1ff9fe68"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 40 --model_name \"345M\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 16:41:22.955542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-07 16:41:24.669892: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-05-07 16:41:24.670875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-05-07 16:41:24.710649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:24.711541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-05-07 16:41:24.711623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-07 16:41:24.714088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-07 16:41:24.714159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-05-07 16:41:24.715727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-07 16:41:24.716084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-07 16:41:24.717652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-07 16:41:24.718121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-05-07 16:41:24.718294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-05-07 16:41:24.718404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:24.718975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:24.719512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-05-07 16:41:24.720055: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-05-07 16:41:24.720174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:24.720741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-05-07 16:41:24.720779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-07 16:41:24.720806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-07 16:41:24.720826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-05-07 16:41:24.720843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-05-07 16:41:24.720862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-05-07 16:41:24.720885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-05-07 16:41:24.720910: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-05-07 16:41:24.720934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-05-07 16:41:24.720997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:24.721608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:24.722136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-05-07 16:41:24.722184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-07 16:41:25.264423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-05-07 16:41:25.264483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-05-07 16:41:25.264501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-05-07 16:41:25.264701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:25.265355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:25.265915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-05-07 16:41:25.266433: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-05-07 16:41:25.266486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13994 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:60: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "2021-05-07 16:41:32.204851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "2021-05-07 16:41:32.519895: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n",
            "Model prompt >>> Freud\n",
            "2021-05-07 16:41:43.896934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-05-07 16:41:44.438947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "======================================== SAMPLE 1 ========================================\n",
            "'s nymphomania to David Lynch's 'Twin Peaks', Dutch murder mystery to Thomas Keneally, Fifa corruption investigative journalism is everywhere. But inevitably, we get into to mudslinging, guilt trips, and starting online petitions (or tweets, if you are really into the counterproductive Snoopy-huttering well ladder behaviour). And organiser of this challenge, David Cameron has so far offered to reschedule the review screening that is due to take place in the House of Commons next month, adding legitimacy to this celebrity madness of internet witch-hunts and obsessive witchhunting that rings alarmingly true in equal measure.\n",
            "\n",
            "And, of course, there is certainly no need to resort to such crazy iconoclastic activism for protecting any sporting event, such is our current obsession with online hoaxes. Media, let the house of it begin. It gets easier to get the coveted negative clap. Of course we treat hoaxes more positively now, returners have the option to reject a scan, and the entertainment industry is more likely to respect creativity when it chimes with our hacked notions of 'pop culture'. Well now it does!\n",
            "\n",
            "Photograph: Knot.<|endoftext|>Texas puppy mauls 12-year-old boy\n",
            "\n",
            "The injuries were so bad, one witness told us, that it looked as though a dog had choked and tied the boy to the trunk. The boy collapsed on a concrete patio, and a local police officer heard a man walk by and unknowingly find some plastic baggies he threw over the girl's face.<|endoftext|>For football fans across the pond, or the baseball sequel fans keep clamoring for, it seems a sneaker model to match the Michelin-starred vegan racing fashion chain Stéphane Weile Magnotta has finally made good on its slogan \"You Have to Love a Stig\" is on the way for the new space hotel sports complex, RAD. The news comes from ourlist even before we saw The Address, and while the building project became known to many in March 2016, it was Tangiers A/S who announced the plans to host football Fan Expo Helsinki featuring their Stefano Garces look-alike suitable for celebratory days off. A couple photos of the luxury hotel will be made available after the season begins on May 1st, but one distinctly started relatively late: The company step into the kitchen ahead of an interview AFTER they had requested be released, blanketing the kitchen with lavish kitchen photos to serve as old-school dress codes\n",
            "================================================================================\n",
            "Model prompt >>> Jung\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Yuk Min. The whimsical HUD Neutron explosive entertainment makes the purchaser think there's a lot going on 'cause the bullet is exploding a few feet away from the muzzle and suffering a weaker burst. Next time look for a way to prop that bullet up into some bright future opportunity. Not A Big Deal That shot a serving wife not really meant anything to moderator Atheism 20 testified. Only Palit thinks so, let's hope the UN scolds him along the lines of Stupid Appeals Part 4 Family software should be mirror thoughts, even extremely good ones. Okay, next time tell me how great one-sided evidence from that motherfucker is while I saw it and where I stood on that constitutional issue. Quadrant Another story holder reports Soviet scent. Enough of this rambling of the steps toward door #4 of our hypothetical passage of the grand jury mandate to stay calm and lemme explain what happened. People say this lack of discipline is really disintermediating, telling me my man gets frustrated once...all he wants to do is annoy me. First, send us a picture as a rule when he's no longer allowed to talk about getting some technical documents. Then proceed to point out how frustrating it is that he is not being greeted with DekSharlet welcome sixteen Nike eye souls consensus defeat Center GTX700 and accompanying gear fails to block my LTE aperture test whether it's bad or not The Secretary of State of Norway said your speech wasn't credible and regretted an aspect of the Norwegian Parliament's Proposal that came from Norway. She then forgave you! Fool Nobles stare benevolently at the reddest crying girl on earth. One of those things that you don't want giggling catophiliacs saying to you in public When idiots like monks lose for the first time in six months the power outage, transcendental arrogance will likely trickle into your two seats in the Grand Jury Committee. Friends are a very precious thing to me. While I can only wish I were able to or get if there's few left grumps of this ilk I leave as an edifice opener the astonished \"settling the client\" process it doesn't matter, we have a tangled David walking out on friendly corner 1dLiche Ulife 2014.\n",
            "\n",
            "Using hyper slightly.\"nSoft not Spitonput_static_USSoccer converted terrorist Monday relief.dosed-entity from human downwashers 2016.Our next post! David's Notes 2016 and the League of Sinderman List The League of Sinderman will be compiling an a\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 5588, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 91, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 681, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 88, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1645, in __exit__\n",
            "    close_thread.join(30.0)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1048, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeDhY97XMDXn"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBaj2L_KMAgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef71df6-441f-49f3-f14b-a797b499884b"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py -- --help"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'src/interactive_conditional_samples.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8rSqkGxg5OK"
      },
      "source": [
        "Generate unconditional samples from the model,  if you're using 345M, add \"--model-name 345M\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaQUEnRxWc3c"
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --model_name \"345M\" | tee /tmp/samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM1Hag-JL3Bt"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdxfye-SL66I"
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py -- --help"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}